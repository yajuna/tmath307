{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsjC+xuUPW2EPf5PcFb0ja",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yajuna/tmath307/blob/master/DE_with_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning, in particular neural networks, have recently been applied in scientific computing problems. This notebook plays with the diea, and shows an example of solving a ordinary differential equation with tensorflow, given initial condition. The code is adapted from (PINNs)\n",
        "\n",
        "https://github.com/janblechschmidt/PDEsByNNs"
      ],
      "metadata": {
        "id": "0riYN3FZW_Dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider an example ODE\n",
        "\n",
        "$\\frac{dy}{dt} = ay$\n",
        "\n",
        "with initial condition $y(0)=2$.\n",
        "\n",
        "We solve the problem on the interval [0,2].\n",
        "\n",
        "With the idea of PINN, we aim to minimize the residual $\\frac{dy}{dt} - ay$"
      ],
      "metadata": {
        "id": "SG1X0ynGabg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy and Tensorflow\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CHfecdculc0X"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set date type\n",
        "DTYPE = 'float32'\n",
        "tf.keras.backend.set_floatx(DTYPE)\n",
        "\n",
        "EPOCH = 5000\n",
        "\n",
        "# set constant- need to be tensors\n",
        "pi = tf.constant(np.pi, dtype = DTYPE)\n",
        "a = pi\n",
        "t_0 = tf.constant([0], dtype = DTYPE)\n",
        "y_0 = tf.constant([2.], dtype=DTYPE)\n",
        "N_0 = 50\n",
        "time_interval = [0., 2.]\n",
        "\n",
        "# convert constants to tensor\n",
        "tmin = tf.constant(time_interval[0], dtype=DTYPE)\n",
        "tmax = tf.constant(time_interval[1], dtype=DTYPE)\n",
        "\n",
        "# define the residual\n",
        "\n",
        "def residual(t, y, y_t):\n",
        "  return y_t - a * y"
      ],
      "metadata": {
        "id": "CaWiTtGVaTo_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "cgJAsVI4W6yM"
      },
      "outputs": [],
      "source": [
        "# if time (t) defined as a random sampling solve the DE correctly, this shows the true power of NN\n",
        "# unfortunately with random points the algorithm does not converge.\n",
        "# t = tf.random.uniform((N_0,1), tmin, tmax, dtype=DTYPE)\n",
        "\n",
        "# use linear space\n",
        "t = tf.linspace(tmin, tmax, N_0)\n",
        "\n",
        "# print(type(time))\n",
        "\n",
        "# print(time.shape)\n",
        "\n",
        "# for j in range(50):\n",
        "#   print(time[j+1] - time[j])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up model.\n",
        "\n",
        "This is a sequential model, set up by adding layers one by one.\n",
        "\n",
        "The variable t is first scaled to be from [tmin, tmax] to [0,1]. This is to help the activation function.\n",
        "\n",
        "The model has 8 hidden layer, with 20 neurons each. The input is 1 dimensional, and takes time; the output is 1 dimensional, and outputs the function $y(t)$."
      ],
      "metadata": {
        "id": "5QHUXwyIpqfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_model(num_hidden_layers=8, num_neurons_per_layer=20):\n",
        "    # Initialize a feedforward neural network\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Input is one-dimensional (time)\n",
        "    model.add(tf.keras.Input(1))\n",
        "\n",
        "    # Introduce a scaling layer to map input to [tmin, tmax]\n",
        "    scaling_layer = tf.keras.layers.Lambda(\n",
        "                lambda t: 2.0*(t - tmin)/(tmax - tmin) - 1.0)\n",
        "    model.add(scaling_layer)\n",
        "\n",
        "    # Append hidden layers\n",
        "    for _ in range(num_hidden_layers):\n",
        "        model.add(tf.keras.layers.Dense(num_neurons_per_layer,\n",
        "            activation=tf.keras.activations.get('tanh'),\n",
        "            kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Output is one-dimensional\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "0oYZYu5tprHq"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute loss and gradient."
      ],
      "metadata": {
        "id": "_9XvLFguyaGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_residual(model, t):\n",
        "\n",
        "    # A tf.GradientTape is used to compute derivatives in TensorFlow\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "        # Variables t is watched during tape\n",
        "        # to compute derivatives y_t\n",
        "        tape.watch(t)\n",
        "\n",
        "        # Determine residual\n",
        "        y = model(t)\n",
        "\n",
        "    y_t = tape.gradient(y, t)\n",
        "\n",
        "    del tape\n",
        "\n",
        "    return residual(t, y, y_t)"
      ],
      "metadata": {
        "id": "hj_IV22wyf1S"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute loss"
      ],
      "metadata": {
        "id": "3B7rjbJ_4mTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(model, t, t_0, y_0):\n",
        "\n",
        "    # Compute mean square of residual\n",
        "    r = get_residual(model, t)\n",
        "    phi_r = tf.reduce_mean(tf.square(r))\n",
        "\n",
        "    # first part of loss\n",
        "    loss = phi_r\n",
        "\n",
        "    # Add loss at initial condition\n",
        "\n",
        "    y_pred = model(t_0)\n",
        "    loss += tf.reduce_mean(tf.square(y_0 - y_pred))\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "I8aqKPbJ4n3s"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the gradient of the loss function (computed above) with respect to the unknown variables in the model (trainable_variables in Tensorflow). This is also done via GradientTape, but now it keeps track of the parameters (weights, biases) in our model. These parameters can be accessed by model.trainable_variables."
      ],
      "metadata": {
        "id": "-ZAsa4nNkrAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = init_model()\n",
        "loss = compute_loss(model, t, t_0, y_0)"
      ],
      "metadata": {
        "id": "9bfe-A-RnIJU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grad_of_loss(model, t, t_0, y_0):\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # This tape is for derivatives with\n",
        "        # respect to trainable variables\n",
        "        tape.watch(model.trainable_variables)\n",
        "        loss = compute_loss(model, t, t_0, y_0)\n",
        "\n",
        "    g = tape.gradient(loss, model.trainable_variables)\n",
        "    del tape\n",
        "\n",
        "    return loss, g"
      ],
      "metadata": {
        "id": "Ro1O_vk7lMRd"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up optimizer and train model.\n",
        "\n",
        "Next we initialize the model, set the learning rate to the step function\n",
        "\n",
        "$$\n",
        "\\delta(n) = 0.01 \\, \\textbf{1}_{\\{n < 1000\\}} + 0.001 \\, \\textbf{1}_{\\{1000 \\le n < 3000\\}} + 0.0005 \\, \\textbf{1}_{\\{3000 \\le n\\}}\n",
        "$$\n",
        "\n",
        "which decays in a piecewise constant fashion, and set up a `tf.keras.optimizer` to train the model. Notice the learning rate decreases as the training goes on."
      ],
      "metadata": {
        "id": "-SQmSbzSnTj9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "XFmQx3yRZEkp"
      },
      "outputs": [],
      "source": [
        "# Initialize model aka y_\\theta\n",
        "model = init_model()\n",
        "\n",
        "# We choose a piecewise decay of the learning rate, i.e., the\n",
        "# step size in the gradient descent type algorithm\n",
        "# the first 1000 steps use a learning rate of 0.01\n",
        "# from 1000 - 3000: learning rate = 0.001\n",
        "# from 3000 onwards: learning rate = 0.0005\n",
        "\n",
        "lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,3000],[1e-2,1e-3,5e-4])\n",
        "\n",
        "# Choose the optimizer\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model for  ð‘=5000  epochs. Here, we set up a function `train_step()` which performs one training step.\n",
        "\n",
        "Note: The `@tf.function` is a so-called `Decorator` within Python. This particular decorator redefines the function that follows, in our case `train_step`, as a TensorFlow graph which may speed up the training significantly."
      ],
      "metadata": {
        "id": "R2mgdzsvoj1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "# add function wrapper to increase speed of training\n",
        "@tf.function\n",
        "def train_step():\n",
        "    # Compute current loss and gradient w.r.t. parameters\n",
        "    loss, grad_theta = get_grad_of_loss(model, t, t_0, y_0)\n",
        "\n",
        "    # Perform gradient descent step\n",
        "    optim.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Number of training epochs\n",
        "N = EPOCH\n",
        "hist = []\n",
        "\n",
        "# Start timer\n",
        "t0 = time()\n",
        "\n",
        "for i in range(N+1):\n",
        "\n",
        "    loss = train_step()\n",
        "\n",
        "    # Append current loss to hist\n",
        "    hist.append(loss.numpy())\n",
        "\n",
        "    # Output current loss after 50 iterates\n",
        "    if i%500 == 0:\n",
        "        print('It {:05d}: loss = {:10.8e}'.format(i,loss))\n",
        "\n",
        "# Print computation time\n",
        "print('\\nComputation time: {} seconds'.format(time()-t0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Mck9DYanW2_",
        "outputId": "33af7a4e-e1cd-4883-aa92-5569565712ed"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It 00000: loss = 3.82497787e+00\n",
            "It 00500: loss = 2.69024992e+00\n",
            "It 01000: loss = 2.67602754e+00\n",
            "It 01500: loss = 2.69462204e+00\n",
            "It 02000: loss = 2.69129848e+00\n",
            "It 02500: loss = 2.68785810e+00\n",
            "It 03000: loss = 2.67749548e+00\n",
            "It 03500: loss = 2.66565895e+00\n",
            "It 04000: loss = 2.51357365e+00\n",
            "It 04500: loss = 7.08898306e-01\n",
            "It 05000: loss = 6.57547057e-01\n",
            "\n",
            "Computation time: 12.694133996963501 seconds\n"
          ]
        }
      ]
    }
  ]
}